<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_list_1-3>li:before{content:"\0025cf   "}.lst-kix_list_1-4>li:before{content:"\0025cb   "}ul.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_1-7>li:before{content:"\0025cf   "}.lst-kix_list_1-5>li:before{content:"\0025a0   "}.lst-kix_list_1-6>li:before{content:"\0025cf   "}ul.lst-kix_list_1-3{list-style-type:none}.lst-kix_list_1-0>li:before{content:"\0025cf   "}ul.lst-kix_list_1-4{list-style-type:none}.lst-kix_list_1-8>li:before{content:"\0025cf   "}ul.lst-kix_list_1-1{list-style-type:none}ul.lst-kix_list_1-2{list-style-type:none}ul.lst-kix_list_1-7{list-style-type:none}.lst-kix_list_1-1>li:before{content:"\0025cb   "}.lst-kix_list_1-2>li:before{content:"\0025a0   "}ul.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_1-5{list-style-type:none}ul.lst-kix_list_1-6{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c20{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#e8f0fe;border-left-style:solid;border-bottom-width:1pt;width:70pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c30{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#2e75b6;border-left-style:solid;border-bottom-width:1pt;width:183pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c21{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#f5f5f5;border-left-style:solid;border-bottom-width:1pt;width:183pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c9{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#f5f5f5;border-left-style:solid;border-bottom-width:1pt;width:140pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c40{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#2e75b6;border-left-style:solid;border-bottom-width:1pt;width:140pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c35{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#ffffff;border-left-style:solid;border-bottom-width:1pt;width:183pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c15{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#f5f5f5;border-left-style:solid;border-bottom-width:1pt;width:45pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c23{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#ffffff;border-left-style:solid;border-bottom-width:1pt;width:140pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c18{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#ffffff;border-left-style:solid;border-bottom-width:1pt;width:45pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c31{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#f5f5f5;border-left-style:solid;border-bottom-width:1pt;width:55pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c7{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#2e75b6;border-left-style:solid;border-bottom-width:1pt;width:45pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c29{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#fff8e1;border-left-style:solid;border-bottom-width:1pt;width:70pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c3{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#ffffff;border-left-style:solid;border-bottom-width:1pt;width:55pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c19{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#e8f5e9;border-left-style:solid;border-bottom-width:1pt;width:70pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c43{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;background-color:#2e75b6;border-left-style:solid;border-bottom-width:1pt;width:55pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c10{border-right-style:solid;padding:3pt 5pt 3pt 5pt;border-bottom-color:#bbbbbb;border-top-width:1pt;border-right-width:1pt;border-left-color:#bbbbbb;vertical-align:top;border-right-color:#bbbbbb;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:398pt;border-top-color:#bbbbbb;border-bottom-style:solid}.c12{color:#2e75b6;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Arial";font-style:normal}.c38{color:#1a1a1a;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c26{color:#444444;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11.5pt;font-family:"Arial";font-style:normal}.c0{padding-top:0pt;padding-bottom:3pt;line-height:1.0;orphans:2;widows:2;text-align:left;height:11pt}.c27{padding-top:0pt;padding-bottom:4pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c22{padding-top:0pt;padding-bottom:6pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left}.c33{padding-top:18pt;padding-bottom:10pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c37{padding-top:0pt;padding-bottom:10pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c36{padding-top:0pt;padding-bottom:2pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c13{padding-top:14pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c39{padding-top:0pt;padding-bottom:20pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.0999999999999999;orphans:2;widows:2;text-align:left}.c24{padding-top:10pt;padding-bottom:5pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c32{font-size:24pt;font-family:"Arial";color:#1a1a1a;font-weight:700}.c11{border-spacing:0;border-collapse:collapse;margin-right:auto}.c44{font-size:14pt;font-family:"Arial";color:#666666;font-weight:400}.c28{font-size:9pt;font-family:"Arial";color:#ffffff;font-weight:700}.c4{font-size:10pt;font-weight:700;font-family:"Arial"}.c16{font-size:10.5pt;font-weight:400;font-family:"Arial"}.c34{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c5{font-size:10pt;font-weight:400;font-family:"Arial"}.c8{font-size:9pt;font-weight:400;font-family:"Arial"}.c14{font-size:10.5pt;font-weight:700;font-family:"Arial"}.c45{font-size:11pt;font-family:"Arial";font-weight:400}.c25{color:#999999}.c42{font-style:italic}.c17{height:0pt}.c41{height:11pt}.title{padding-top:0pt;color:#000000;font-size:28pt;padding-bottom:0pt;font-family:"Arial";line-height:1.0;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.0;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:18pt;color:#1a1a1a;font-weight:700;font-size:18pt;padding-bottom:10pt;font-family:"Arial";line-height:1.0;orphans:2;widows:2;text-align:left}h2{padding-top:14pt;color:#2e75b6;font-weight:700;font-size:13pt;padding-bottom:6pt;font-family:"Arial";line-height:1.0;orphans:2;widows:2;text-align:left}h3{padding-top:10pt;color:#444444;font-weight:700;font-size:11.5pt;padding-bottom:5pt;font-family:"Arial";line-height:1.0;orphans:2;widows:2;text-align:left}h4{padding-top:0pt;color:#2e74b5;font-size:11pt;padding-bottom:0pt;font-family:"Arial";line-height:1.0;font-style:italic;orphans:2;widows:2;text-align:left}h5{padding-top:0pt;color:#2e74b5;font-size:11pt;padding-bottom:0pt;font-family:"Arial";line-height:1.0;orphans:2;widows:2;text-align:left}h6{padding-top:0pt;color:#1f4d78;font-size:11pt;padding-bottom:0pt;font-family:"Arial";line-height:1.0;orphans:2;widows:2;text-align:left}</style></head><body class="c34 doc-content"><p class="c27"><span class="c32">mcfeedback</span></p><p class="c36"><span class="c44">Murray-Claude Feedback Algorithm</span></p><p class="c37"><span class="c25 c45">Phase 1 Research Journal &mdash; Experiments 1&ndash;13</span></p><p class="c36"><span class="c5 c25">February 2026</span></p><p class="c39"><span class="c5 c25 c42">Murray &amp; Claude</span></p><h1 class="c33"><span class="c38">Architecture</span></h1><p class="c22"><span class="c16">mcfeedback implements biologically-plausible learning without backpropagation. Synapses learn using only local signals: eligibility traces, chemical diffusion from modulatory neurons, and an ambient spatial field. There are no global gradients.</span></p><h3 class="c24"><span class="c26">Network topology</span></h3><p class="c22"><span class="c16">Two recurrent clusters of 30 neurons each (60 total). Each cluster contains 5 special-role neurons (input or output), 2 modulatory neurons, and 23 regular neurons. This is not a layered feedforward network &mdash; it is a recurrent soup with probabilistic all-to-all connectivity. Intra-cluster connection probability: 60%. Inter-cluster: 50%. No self-loops. Single forward propagation pass per training step (signals travel exactly one hop).</span></p><h3 class="c24"><span class="c26">Task</span></h3><p class="c22"><span class="c16">Learn 4 binary inversion patterns with 5 input and 5 output neurons. P1: [1,0,1,0,1]&rarr;[0,1,0,1,0]. P2: [1,1,0,0,1]&rarr;[0,0,1,1,0]. P3: [0,1,1,0,0]&rarr;[1,0,0,1,1]. P4: [0,1,0,1,0]&rarr;[1,0,1,0,1]. Note: P1 and P4 are exact inverses of each other. Random chance = 50%.</span></p><h3 class="c24"><span class="c26">Three learning mechanisms</span></h3><p class="c22"><span class="c14">Eligibility traces (four-quadrant flagging):</span><span class="c16">&nbsp;Co-activation of pre and post neurons &rarr; positive trace. Mismatch (one fires, other doesn&rsquo;t) &rarr; negative trace. Co-silence in an active area &rarr; weak positive. Irrelevant silence &rarr; zero.</span></p><p class="c22"><span class="c14">Chemical diffusion:</span><span class="c16">&nbsp;Modulatory neurons broadcast reward/punishment spatially with 1/distance falloff. Radius: 15 units (spans both clusters in Full model). The chemical level modulates the magnitude of weight updates.</span></p><p class="c22"><span class="c14">Dampening filters:</span><span class="c16">&nbsp;Activity history tracking, information dampening (inverted-U response peaking at target fire rate), and ambient relevance field (3-unit radius, intra-cluster only).</span></p><h3 class="c24"><span class="c26">Four experimental conditions</span></h3><p class="c22"><span class="c14">Baseline:</span><span class="c16">&nbsp;Global chemical (constant dose everywhere), no ambient field, no dampening. </span><span class="c14">Ambient only:</span><span class="c16">&nbsp;Adds spatial ambient field. </span><span class="c14">Dampening only:</span><span class="c16">&nbsp;Adds activity-based dampening filters. </span><span class="c14">Full model:</span><span class="c16">&nbsp;All mechanisms active with local chemical diffusion.</span></p><h3 class="c24"><span class="c26">Evaluation protocol</span></h3><p class="c22"><span class="c16">Frozen-weight evaluation established from iteration 2 onward. After training, weights are frozen and a forward pass with no learning produces the accuracy measurement. This prevents inflated scores from eval-during-learning artifacts. 10 seeds (42, 137, 271, 314, 500, 618, 777, 888, 999, 1234), 1000 training episodes per run unless noted.</span></p><hr style="page-break-before:always;display:none;"><p class="c1 c41"><span class="c2"></span></p><h1 class="c33"><span class="c38">Experiments</span></h1><h2 class="c13"><span class="c12">Iteration 1 &mdash; Baseline multi-seed</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Establish baseline performance across all four conditions with proper statistical power (N=10 seeds).</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Full model: 45.5% &plusmn; 5.0% (p=0.006, significantly worse than Baseline at 53.5%). Training-time accuracy of ~80% was an artifact of eval-during-learning.</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">No learning detected. Dampening actively harmful. Led to frozen-weight evaluation protocol.</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><h2 class="c13"><span class="c12">Iteration 3 &mdash; Flipped eligibility signs</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Test whether inversion task requires anti-Hebbian learning. Flip coActivation from +1 to &minus;1 and mismatch from &minus;0.5 to +0.5.</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Full model: 46.0% &plusmn; 3.2%. Variance collapsed &mdash; 9/10 seeds locked at exactly 45%. Per-pattern analysis showed flags homogenizing outputs rather than differentiating them.</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Sign flip erased information rather than enhancing it. Flags are making things more uniform, not more selective.</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><h2 class="c13"><span class="c12">Iteration 4 &mdash; Flag gate (strengthen/decay)</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Add flag persistence: flags must accumulate over consecutive turns (gain=0.3, decay=0.7, threshold=0.5) before gating weight updates. ~2 consistent turns required to latch.</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Full model: 53.0% &plusmn; 9.2%, max 65% (3 seeds). Bimodal: seeds 42&ndash;500 stall at 45%, seeds 888&ndash;1234 reach 55&ndash;65%. First above-chance results.</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">First meaningful progress. Flag persistence broke the 45% attractor for some seeds. Became the reference configuration for all subsequent experiments.</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><h2 class="c13"><span class="c12">Iteration 5 &mdash; Squared reward</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Squash reward signal near 50% accuracy toward zero, making fixed-output strategies break even. rewardExponent=2.0 so 55% accuracy &rarr; reward 0.01 instead of 0.10.</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Full model: 45.0% &plusmn; 8.2% (p=0.03, significantly worse). The squared reward starved early learning signal &mdash; a network at 60% gets reward 0.04 instead of 0.20, not enough to bootstrap.</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Hypothesis rejected. Squared reward suppresses everything near 50%, including genuine early improvement. Flag gate + squared reward form a deadlock: both need signal to bootstrap.</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><h2 class="c13"><span class="c12">Iteration 6 &mdash; Reward annealing</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Smooth blend from linear to squared reward over training. Linear ep 0&ndash;349, blend ep 350&ndash;700, pure squared ep 700+.</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Full model: 51.0% &plusmn; 10.7%. Same 3 seeds succeed, same 7 fail. Worst variance yet. Partial recovery vs iter 5 but no improvement over iter 4.</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Reward shape is not the bottleneck. Three experiments (linear, squared, annealed) all produce the same seed-dependent success pattern. The problem is upstream of reward.</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><h2 class="c13"><span class="c12">Iteration 7 &mdash; Connectivity diagnostic</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Measure structural differences between succeeding and failing seeds: direct I&rarr;O connections, 2-hop paths, fan-out/fan-in, chemical dose budget, modulatory neuron distance.</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Clean null result. All metrics overlap completely between good and poor seeds. Direct I&rarr;O: 13.6 vs 12.2 (noise). Chemical dose: 1.13 vs 1.16 (negligible). 2-hop paths: 344 vs 349.</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">The failure mode is not structural. Network wiring is statistically identical across seeds. The divergence must emerge from early training dynamics, not topology.</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><h2 class="c13"><span class="c12">Iteration 8 &mdash; Flag strength diagnostic</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Log flag strength at episodes 100, 300, 500. Prediction: good seeds latch flags early, poor seeds don&rsquo;t.</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Prediction was wrong. Both groups have 87&ndash;95% of flags latched by episode 100 with mean |flagStrength| &asymp; 0.93. Groups are statistically indistinguishable.</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Flags are saturated, not selective. With gain=0.3, any synapse seeing two consecutive non-zero traces latches. The gate opens for 90%+ of synapses uniformly &mdash; it provides no discrimination.</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><h2 class="c13"><span class="c12">Iteration 9 &mdash; Flag gate warmup</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Bypass flag gate for first 200 episodes (all synapses learn freely), then activate normal gating.</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Full model: 45.5% &plusmn; 5.0% (p&lt;0.01, significantly worse). Variance collapsed to &plusmn;5% &mdash; network finds the same poor attractor on every seed.</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Unfiltered early learning commits to bad fixed-output weights. Flags then lock in those bad weights. Opening the gate early is the problem, not the cure.</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><hr style="page-break-before:always;display:none;"><p class="c1 c41"><span class="c2"></span></p><h2 class="c13"><span class="c12">Iteration 10 &mdash; Direction-consistent flags</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Replace simple flag accumulation with a consistency detector. Track consecutive same-sign traces (threshold=5), sharp penalty on direction flip (0.5&times;), slower gain (0.15).</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Full model: 49.0% &plusmn; 7.0% (p&lt;0.05 worse). Latch rates dropped to 68&ndash;93% (from 90%+). Seeds with lower latch rates (42, 1234) performed better, but 8/10 seeds still saturated at 86&ndash;93%.</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Partial success on selectivity &mdash; mechanism correctly identifies that lower saturation correlates with better performance. But the task dynamics still satisfy the 5-turn streak requirement trivially for most synapses.</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><h2 class="c13"><span class="c12">Iteration 10b &mdash; Propagation cycles</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Run 3 accumulate-and-fire cycles per training step so signals can travel multiple hops through the recurrent graph.</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Full model: 46.5% &plusmn; 6.7% (p&lt;0.01). Flag saturation finally broken (25&ndash;81%), but accuracy worse. Seed 888 (33% latch, 65% acc) confirms low saturation helps, but 9 other seeds still fail.</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Propagation cycles successfully broke flag saturation but introduced noise through compound signal amplification. Also discovered: homeostasis runs 3&times; per episode inside the loop &mdash; an implementation issue to fix.</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><h2 class="c13"><span class="c12">Iteration 11 &mdash; Synapse frustration flip</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Per-synapse frustration detection: track same-direction weight movement and average chemical. After 30 consecutive same-direction steps with negative reward, partially reverse the weight.</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Full model: 48.0% &plusmn; 4.8% (p&lt;0.01). Mechanism is all-or-nothing: 8/10 seeds had zero flips, 2/10 had every synapse flip (cascade). Key finding: 6/10 seeds have mean |weight| &lt; 0.28 &mdash; barely moved from initialization.</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Cascade problem: one flip destabilizes neighbors, triggering chain reaction. But the real discovery was weight starvation &mdash; most synapses decay faster than they learn. Weight decay (0.005/step) exceeds typical learning delta (0.001/step).</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><h2 class="c13"><span class="c12">Iteration 12 &mdash; Learning/decay balance</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Three sub-experiments: (a) double learning rate, (b) halve weight decay, (c) both. Test whether weight starvation is the fundamental bottleneck.</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">012b (0.5&times; weight decay): 55.0% &plusmn; 0.0% &mdash; ALL 10 seeds at 55%. First time 100% seed coverage achieved. Mean |weight| jumped from 0.30 to 0.79. 012c (both changes): same accuracy, |weight|=1.44.</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Weight starvation was the core problem all along. One parameter change solved what 8 experiments of mechanism tuning couldn&rsquo;t. New ceiling at 55% is structural: network learns 3/4 patterns (P2&ndash;P4 at 60%) but systematically fails P1 (40%), because P1 and P4 are exact inverses sharing the same synapses.</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><h2 class="c13"><span class="c12">Iteration 13 &mdash; 012b + propagation cycles</span></h2><table class="c11"><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c1"><span class="c4">Aim</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Combine the weight decay fix with propagationCycles=3 to enable multi-hop signal flow, potentially breaking the 55% ceiling through hidden representations.</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1"><span class="c4">Result</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Full model: 45.0% &plusmn; 0.0% &mdash; complete collapse, 0/10 seeds above 45%. Mean |weight| dropped to 0.35. Per-pattern bias flipped: P1=60%, P2&ndash;P4=40% (mirror of 012b).</span></p></td></tr><tr class="c17"><td class="c19" colspan="1" rowspan="1"><p class="c1"><span class="c4">Conclusion</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c6"><span class="c5">Threshold homeostasis runs inside the propagation loop, so 3 cycles = 3&times; homeostasis speed. This drives the network to the wrong attractor. Fix: move regulateThreshold() outside the propagation loop. The 012b configuration (halved weight decay, single propagation) remains the best result.</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><hr style="page-break-before:always;display:none;"><p class="c1 c41"><span class="c2"></span></p><h1 class="c33"><span class="c38">Progress summary</span></h1><p class="c22"><span class="c16">Full model mean accuracy and key observations across all iterations:</span></p><p class="c0"><span class="c2"></span></p><table class="c11"><tr class="c17"><td class="c7" colspan="1" rowspan="1"><p class="c1"><span class="c28">Iter</span></p></td><td class="c40" colspan="1" rowspan="1"><p class="c1"><span class="c28">Change</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c1"><span class="c28">Mean</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c1"><span class="c28">Max</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c1"><span class="c28">Notes</span></p></td></tr><tr class="c17"><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 1</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c1"><span class="c8">Original flags, linear reward</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1"><span class="c8">45.5%</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">55%</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c1"><span class="c8">6/10 fail</span></p></td></tr><tr class="c17"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 3</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c1"><span class="c8">Flipped eligibility signs</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c1"><span class="c8">46.0%</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">55%</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c1"><span class="c8">No change</span></p></td></tr><tr class="c17"><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 4</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c1"><span class="c8">Flag gate (strengthen/decay)</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1"><span class="c8">53.0%</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">65%</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c1"><span class="c8">3/10 escape</span></p></td></tr><tr class="c17"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 5</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c1"><span class="c8">Squared reward</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c1"><span class="c8">45.0%</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">65%</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c1"><span class="c8">Regression</span></p></td></tr><tr class="c17"><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 6</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c1"><span class="c8">Reward annealing</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1"><span class="c8">51.0%</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">65%</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c1"><span class="c8">Partial recovery</span></p></td></tr><tr class="c17"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 7</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c1"><span class="c8">Connectivity diagnostic</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c1"><span class="c8">&mdash;</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">&mdash;</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c1"><span class="c8">Null: structure not the cause</span></p></td></tr><tr class="c17"><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 8</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c1"><span class="c8">Flag strength diagnostic</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1"><span class="c8">&mdash;</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">&mdash;</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c1"><span class="c8">Flags saturated at 90%+</span></p></td></tr><tr class="c17"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 9</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c1"><span class="c8">Flag gate warmup</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c1"><span class="c8">45.5%</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">55%</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c1"><span class="c8">Worse (p&lt;0.01)</span></p></td></tr><tr class="c17"><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 10</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c1"><span class="c8">Direction-consistent flags</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1"><span class="c8">49.0%</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">65%</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c1"><span class="c8">Partial signal</span></p></td></tr><tr class="c17"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 10b</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c1"><span class="c8">+ propagationCycles=3</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c1"><span class="c8">46.5%</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">65%</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c1"><span class="c8">Broke saturation, added noise</span></p></td></tr><tr class="c17"><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 11</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c1"><span class="c8">Synapse frustration flip</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1"><span class="c8">48.0%</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">55%</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c1"><span class="c8">Cascade problem</span></p></td></tr><tr class="c17"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 12b</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c1"><span class="c8">0.5&times; weight decay</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c1"><span class="c8">55.0%</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">55%</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c1"><span class="c8">10/10 seeds &#10003;</span></p></td></tr><tr class="c17"><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 12c</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c1"><span class="c8">2&times; LR + 0.5&times; WD</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1"><span class="c8">55.0%</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c1"><span class="c8">55%</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c1"><span class="c8">10/10 seeds &#10003;</span></p></td></tr><tr class="c17"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">Iter 13</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c1"><span class="c8">012b + propagationCycles=3</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c1"><span class="c8">45.0%</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c8">45%</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c1"><span class="c8">Homeostasis bug</span></p></td></tr></table><p class="c0"><span class="c2"></span></p><h1 class="c33"><span class="c38">Key findings</span></h1><p class="c22"><span class="c14">1. Weight starvation was the fundamental bottleneck.</span><span class="c16">&nbsp;For 11 experiments, we tuned flags, reward shaping, and gating mechanisms while the real problem was arithmetic: weight decay outpaced learning by 5&times; for typical synapses. Halving weight decay (iter 12) solved what no mechanism change could.</span></p><p class="c22"><span class="c14">2. The 55% ceiling is a capacity limit, not a learning failure.</span><span class="c16">&nbsp;The network consistently learns 3/4 patterns and sacrifices the 4th (P1), because P1 and P4 are exact inverses sharing the same synapses. A single static weight matrix cannot satisfy both. Breaking past 55% requires context-dependent routing &mdash; different internal states for different inputs.</span></p><p class="c22"><span class="c14">3. Flag saturation is real but secondary.</span><span class="c16">&nbsp;The flag gate latches 90%+ of synapses within 100 episodes regardless of seed. Direction-consistent flagging and propagation cycles can reduce this, but lower saturation alone doesn&rsquo;t improve accuracy.</span></p><p class="c22"><span class="c14">4. Diagnostic experiments are as valuable as mechanism changes.</span><span class="c16">&nbsp;The connectivity diagnostic (iter 7), flag strength diagnostic (iter 8), and weight magnitude analysis (iter 11) each ruled out entire hypothesis branches and redirected effort productively.</span></p><p class="c22"><span class="c14">5. Implementation details dominate.</span><span class="c16">&nbsp;Homeostasis running inside the propagation loop (iter 13), eval-during-learning artifacts (iter 1), and the learning/decay ratio (iter 12) had larger effects than any novel mechanism. Getting the basics right matters more than adding complexity.</span></p><p class="c0"><span class="c2"></span></p><h1 class="c33"><span class="c38">Next steps</span></h1><p class="c22"><span class="c14">1. Fix propagation loop.</span><span class="c16">&nbsp;Move threshold homeostasis outside the propagation cycle loop. Retest propagationCycles=3 on 012b base to see if multi-hop signals enable hidden representations that break the 55% ceiling.</span></p><p class="c22"><span class="c14">2. Genetic algorithm for architecture search.</span><span class="c16">&nbsp;Evolution optimizes structural priors (cluster count, spacing, connectivity, modulatory placement, learning rate, decay); mcfeedback learns within each configuration. This replaces manual parameter tuning with systematic exploration of the fitness landscape.</span></p><p class="c22"><span class="c14">3. Longer training.</span><span class="c16">&nbsp;Run 012b for 5000&ndash;10000 episodes to determine whether the 55% ceiling is a time constraint or a fundamental capacity limit.</span></p><p class="c22"><span class="c14">4. Structured inter-cluster wiring.</span><span class="c16">&nbsp;Replace random inter-cluster connectivity with topographic mapping (input neuron i preferentially connects to output-adjacent neurons). Biology doesn&rsquo;t wire long-range connections randomly.</span></p></body></html>